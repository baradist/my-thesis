\section{Обучение с подкреплением} \label{ch1:ml}

Хотя алгоритмы \hyperref[acr:rl]{RL} эффективно решают различные задачи, им не хватает масштабируемости и размерности.

С ростом глубоких нейронных сетей в последние годы, RL начинает использовать их функции приближения и представления свойств [14]. Это помогает преодолеть недостатки алгоритмов RL.

Это устраняет необходимость описывать свойства вручную, позволяя обучать модели, способные непосредственно выводить оптимальные действия, на основе необработанного и высокоразмерного ввода с сенсоров.

Таким образом, использование ИНС в обучении подкреплением, создает новую область – глубокое обучение с подкреплением (Deep Reinforcement Learning, DRL).

\subsection{Алгоритмы глубокого обучения с подкреплением}

\subsection{Подход, основанный на функции состояния (Value Based подход)}

\subsection{Линия поведения (Policy Based)}

\subsubsection{Policy Gradients}

Policy Gradients содержимое
\textbf{TODO: научиться писать формулы}
\begin{equation} 
	\label{eq:fConcept-order-ch1}
	\begin{multlined}
		(A_1,B_1)\leq (A_2,B_2)\; \Leftrightarrow \\  \Leftrightarrow\; A_1\subseteq A_2\; \Leftrightarrow \\ \Leftrightarrow\; B_2\subseteq B_1. 
	\end{multlined}
\end{equation}