\chapter{Псевдокод алгоритма DDPG}\label{appendix-ddpg-alg}                            % Заголовок

\setlength{\parindent}{0em}

Случайным образом инициализируем сеть критика $Q(s, a|\theta^\mu)$ с весами $\theta^Q$ и $\theta^\mu$

Инициализируем целевую сеть $Q'$ и $\mu'$ весами $\theta^{Q'} \leftarrow \theta^Q$, $\theta^{\mu`'} \leftarrow \theta^\mu$

Инициализируем реплей-буффер $R$

\textbf{for} episode = 1, M \textbf{do}

\setlength{\parindent}{1em}

Инициализируем рандомный процесс $\mathcal{N}$ для исследования

Получаем начальные наблюдаемые состояния $s_l$

\textbf{for} t = 1, T \textbf{do}

\setlength{\parindent}{2em}

Выбираем действие $a_t = \mu (s_t | \theta^\mu) + \mathcal{N}_t$ исходя из текущей политики и исследовательского шума

Выполняем действие $a_t$ и наблюдаем награду $r_t$ и новое состояние $s_{t+1}$

Сохраняем переход $(s_t, a_t, r_t, s_{t+1})$ в R

Семплируем мини-набор из $N$ переходов $(s_i, a_i, r_i, s_{i+1})$ из R

Задаём $y_i = r_i + \gamma Q' (s_{i+1}, \mu' (s_{i+1} | \theta^{\mu'})| \theta^{Q'})$

Обновляем критика минимизацией функции потерь: $L = \dfrac{1}{N} \sum_{i} (y_i - Q(s_i, a_i | \theta^Q))^2$

Обновляем политику актора используя семплированый градиент политики:

\begin{equation}
    \begin{multlined}
        \nabla_{\theta^\mu} \approx \dfrac{1}{N} \sum_{i} \nabla_a Q(s, a | \theta^Q) | _{s=s_i, a=\mu(s_i)} \nabla_{\theta^\mu} \mu (s | \theta^\mu) | _{s_i}
    \end{multlined}
\end{equation}

Обновляем целевые сети:

\begin{equation}
    \begin{multlined}
        \theta^{Q'} \leftarrow \tau \theta^Q + (1 - \tau) \theta^{Q'} \\
        \theta^{\mu'} \leftarrow \tau \theta^\mu + (1 - \tau) \theta^{\mu'}
    \end{multlined}
\end{equation}

\setlength{\parindent}{1em}

\textbf{end for}

\setlength{\parindent}{0em}

\textbf{end for}

\setlength{\parindent}{2.5em}
