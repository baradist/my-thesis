\chapter{Сценарии и эксперименты}

Эксперименты проводятся в мультиагентной среде multiagent-particle-envs \cite{multiagent-particle-envs} от компании OpenAI.


\section{Эксперименты}

Для тестирования был использован компьютер со следующими параметрами:

\begin{itemize}
	\item Процессор: Intel Core i5-8365U 1.6 GHz;
	\item Количество физических ядер: 4;
	\item Количество логических ядер: 8;
	\item Объем оперативной памяти: 16 ГБ.
\end{itemize}

\subsection{Эксперимент на одном мозге}

В алгоритме MADDPG каждый агент имеет набор сетей \textit{критиков}, чтобы иметь собственный механизм оптимизации политики. Тем не менее, два агента в \textit{Simple Reference} полностью симметричны в том смысле, что они оба имеют одинаковое пространство наблюдения, пространство действий и имеют общую глобальную награду. Тем самым агенты формулируют аналогичные оптимальные политики.

Таким образом, каждому агенту можно было бы не создавать свой собственный мозг из сетей критиков, а вместо этого тренировать один мозг для всех агентов. Кроме того, единый мозг MADDPG позволяет агентам общаться на одном «языке», что означает, что они произносят и понимают ориентиры в одних и тех же кодировках.

Также был разработан вариант этого алгоритма. Можно сказать, что в данном случае имеются два мозга: один для агентов из одной команды, второй – для агентов из другой команды. Этот вариант был применён к сценарию \textit{Simple Tag}. Это возможно благодаря тому, что агенты из одной команды в этом сценарии имеют одинаковое пространство наблюдения, пространство действий и имеют общую глобальную награду, но это не так для агентов из разных команд. Поэтому и пришлось использовать отдельный набор сетей для каждой команды.

\subsection{Эксперимент с декомпозированным вознаграждением}

Чтобы обеспечить чёткие сигналы для двух независимых поддействий, была предпринята попытка реализовать отдельные наборы \textit{актор-критиков} для физического движения и общения.

\begin{figure}[ht!]
    \center
    \includegraphics [scale=0.60] {my_folder/images/ch4/actions_and_rewards.png}
    \caption{С левой стороны от пунктирной линии - действие агента, которое представляет собой совокупность физического движения \textit{u} и коммуникационного действия \textit{c}. С правой стороны показано, как награда рассчитывается и назначается для оценки действий. Два агента имеют одинаковую глобальную награду - среднее значение между $r_0$ и $r_1$}
    \label{fig:action-reward}
\end{figure}

Из \firef{fig:action-reward} видно, что физическое перемещение $u_0$ агента 0 оценивается $r_0$, а высказывание $c_0$ - $r_1$. Окончательное вознаграждение затем раскладывается как кортеж из $r_0$ и $r_1$, т.е. $[r_0; r_1]$, причём каждый элемент соответствует сигналу оценки физического движения и связи соответственно. Точно так же, последний набор вознаграждений для агента 1 равен $[r_1; r_0]$.

\begin{figure}[ht!]
    \center
    \includegraphics [scale=0.60] {my_folder/images/ch4/decomposed_reward_structure.png}
    \caption{Сетевая структура декомпозированного вознаграждения по сравнению со стандартным MADDPG}
    \label{fig:decomposed-reward-structure}
\end{figure}

Для физического движения и общения созданы два отдельных набора сетей \textit{акторов-критиков}, чтобы обеспечить согласованные и точные градиенты для оптимизации двух поддействий, см. \firef{fig:decomposed-reward-structure}. Разложения вознаграждения можно рассматривать как параллельное обучение двух слушателей из \textit{Simple Speaker Listener}. Слушатель 1 - это агент 0 с коммуникацией $c_0$ в качестве говорящего и агент 1 с физическим движением $u_1$ в качестве слушателя, а слушатель 2 - агент 1 с $c1$ в качестве говорящего и агент 0 с $u_0$ в качестве слушателя, как показано на \firef{fig:action-reward}.

MADDPG с разложенным вознаграждением может быть хорошо приспособлен только к \textit{Simple Reference}, но не может быть обобщён для других игровых сценариев, поскольку, пространства наблюдений и действий у агентов в этом сценарии симметричны, а главное, каждый агент производит оба вида действия. В большинстве других сценариев это не так. Следовательно, необходимо проводить больше экспериментов с общей глобальной наградой.

\subsection{Эксперимент с обучением по учебной программе}

Обучение по учебной программе применено с целью решения проблемы расходящихся сетей, возникшей в ходе обучения. Примеры обучения организованы в таком порядке, что постепенно вводятся всё более сложные концепции.

Варианты учебной программы на примере сценария \textit{Simple Reference}:

\begin{itemize}
    \item Увеличивать количество ориентиров постепенно. На первом уровне сложности в этой учебной программе цель фиксирована для обоих агентов, скажем, обозначена красным ориентиром. На втором уровне добавляется ещё один ориентир, например, зелёный, и цель выбирается случайным образом из двух ориентиров. Последний уровень - это исходная настройка сценария, т. е. рандомизация цели из трёх ориентиров. Цель этой учебной программы состоит в том, чтобы агенты учились от простого сценария с меньшим количеством ориентиров к более сложному сценарию со всеми ориентирами.
    \item Обучить агента 0 в качестве \textit{говоруна} и агента 1 в качестве \textit{слушателя} на первом этапе, а на следующем этапе обучить двух агентов в качестве \textit{говорунов} и \textit{слушателей} в исходном игровом сценарии.
    \item Предоставить агентам заранее определённые коммуникационные действия для трёх цветов, скажем, [1; 0; 0] для красного, [0; 1; 0] для зелёного и [0; 0; 1] для синего. Таким образом, первая фаза состоит в том, чтобы изучить физическое движение, чтобы правильно двигаться. Затем, одному из агентов больше не предоставляются предопределённые коммуникационные действия, но взамен предоставляется связь с другим агентом (показываются его коммуникационные действия). Ожидается, что другому агенту будет легче научиться правильно общаться. Наконец, задаются исходные игровые настройки для двух агентов, когда изучаются как физическое движение, так и общение.
\end{itemize}

\section{Метрики измерения консистентности коммуникационных действий}

Обучение модели можно ориентировочно оценивать на глаз при рендеринге физического движения, но можно и создать метрику и строить по ней графики. Выбранная метрика для коммуникационных действий заключается в том, что агент должен однозначно интерпретировать комунникационные действия и выбирать соответствующие им цвета цели.

Методика, используемая для измерения сходимости коммуникации, заключается в том, что создаётся матрица, которая сопоставляет интерпретацию агента и текущий цвет цели.

Действие общения --- это вектор из трёх вещественных чисел. Каждой строке матрицы соответствует действие, причём максимальное значение заменено 1, а остальные --- 0.

Тогда после нормализации матрицы, если обучение сходится, она должна выглядеть примерно так:

\begin{equation*}
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
\end{equation*} или

\begin{equation*}
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 0
    \end{bmatrix}
\end{equation*} и т.д.

Неважно, как агенты интерпретируют цвета, \textit{детерминант} этой матрицы, в конце концов, стремится к 1 или -1. В экспериментах данной работы использовался именно этот способ.

Ещё один способ измерения заключается в том, чтобы смотреть, какое действие коммуникации выбрал говорун при наблюдении определённого цвета и проверять, совпадает ли оно с тем, что было выбрано в прошлый раз с тем же наблюдением. График с результатом должен приблизиться к $100\%$ после схождения.

\input{my_folder/ch4/scenarios}
