\section{Сценарии}

\subsection{Сценарий 1. Simple Speaker Listener}  \label{exp-ssl}

Сценарий Simple Speaker Listener воспроизводится и тестируется с использованием алгоритма MADDPG \cite{lowe2017multiagent}.

Сценарий упоминается в разделе \hyperref[intro:ssl]{Вводная глава: Сценарий 1. Simple Speaker Listener}. В этом сценарии два агента имеют разные пространства действий и наблюдений. Наблюдение говоруна $o_s$~--- это цвет цели, обозначенный 3-х канальным вектором $d \in \mathbb{R}^3$. Наблюдение слушателя $o_l$ – это вектор конкатенации его собственной скорости $\upsilon \in \mathbb{R}^2$ его расстояния до трёх ориентиров $p = [p_1, p_2, p_3], p_i \in \mathbb{R}$ и сигнал, произнесённый говоруном на предыдущем временном шаге:

\begin{equation}
    \begin{multlined}
        o_s = [d], \\
        o_l = \begin{bmatrix}
                  \upsilon \\ p \\ c
        \end{bmatrix}.
    \end{multlined}
\end{equation}

Коммуникационное действие говоруна обозначается «one-hot encoding» вектором ${[1; 0; 0]}$, или ${[0; 1; 0]}$, или ${[0; 0; 1]}$ для обозначения трёх ориентиров соответственно.
Физическое действие $u$ слушателя~--- это 5-канальный вектор, каждый из которых представляет одно направление движения (вверх, вниз, влево, вправо или без движения). Наблюдения и действия говоруна и слушателя и их взаимосвязь показаны на рисунке \firef{fig:ch4-ssl}.

\begin{figure}[ht!]
    \center
    \includegraphics [scale=0.60] {my_folder/images/ch4/simple_speaker_listener.png}
    \caption{Говорун наблюдает цвет целевого ориентира d и издаёт коммуникационое действие, которое будет получено слушателем. Слушатель производит физическое движение}
    \label{fig:ch4-ssl}
\end{figure}

Два агента имеют общую награду $r$, которая является отрицательным евклидовым расстоянием между слушателем и его целью. Проблема, которую необходимо решить в этом сценарии, заключается в поиске оптимальных политик для говоруна и слушателя, чтобы максимизировать ожидаемую награду, то есть найти $\max_{\pi}R(\pi)$, где

\begin{equation}
    \begin{multlined}
        R(\pi) = \mathbb{E}[\sum_{t=0}^{T}r(s_t, a_t)].
    \end{multlined}
\end{equation}

Во время обучения говорун учится различать три ориентира и передавать целевой ориентир слушателю. И слушатель должен изучить закодированные высказывания говоруна и перейти к правильной цели.

Архитектура актор-сети говоруна аналогична архитектуре слушателя, каждый из которых содержит два полносвязанных слоя с 64 нейронами и использует функцию активации relu. Однако их выходные слои различаются с точки зрения количества единиц. Сети критиков имеют структуру, аналогичную сетям акторов, за исключением того, что они выдают скалярное $Q$-значение.

На этом сценарии было произведено измерение консистентности коммуникационных действий для алгоритмов DDPG и MADDPG.

\subsection{Сценарий 2. Simple Reference} \label{exp-sr}

Simple Reference~--- это более сложный сценарий, который расширяет Simple Speaker Listener.

Проблема, которую необходимо решить в этом сценарии, была изложена в разделе \hyperref[intro-sr]{Вводная глава: Сценарий 2. Simple Reference}. На \firef{fig-intro-sr} показан сценарий игры и поведение агентов при использовании оптимальных политик. В этом сценарии оба агента одновременно выступают и слушателями, и говорунами, что означает, что они оба выполняют как физические, так и коммуникационные действия. Высказывание каждого агента на одном шаге наблюдается другим агентом на следующем шаге, как показано на \firef{fig:exp-sr}.

Каждый агент наблюдает скорость, расстояние до ориентиров, цвет цели другого агента и высказывание другого агента. Действие каждого агента состоит из двух поддействий: физического движения $u$ и коммуникационного действия $c$.

Наблюдение и действие:

\begin{equation}
    \begin{multlined}
        o_i = \begin{bmatrix}
                  \upsilon \\ p \\ d \\ c
        \end{bmatrix}, \\
        a_i = \begin{bmatrix}
                  u \\ c
        \end{bmatrix}.
    \end{multlined}
\end{equation}


Это показано на \firef{fig:exp-sr}.

Наградой каждого агента является среднее значение отрицательных евклидовых расстояний от агентов до их целей. Таким образом, оцениваются как физические, так и коммуникационные действия агентов. Предположим, что $r_0$~--- это отрицательное евклидово расстояние от агента 0 до его цели, а $r_1$~--- это расстояние от агента 1 до его цели. Окончательная награда для каждого агента:

\begin{equation}
    \begin{multlined}
        r = \frac{r_0 + r_1}{2}.
    \end{multlined}
\end{equation}

Награда $r_0$ используется для оценки физического движения $u_0$ агента 0 и коммуникационного действия $c_1$ агента 1. То же относится и к $r_1$ - он используется для оценки физического движения $u_1$ агента 1 и высказывания $c_0$ агента 0. Это показано на \firef{fig:exp-sr}.

Архитектура нейронных сетей актора-критика двух агентов в этом сценарии аналогична архитектуре в предыдущем сценарии.

\begin{figure}[ht!]
    \center
    \includegraphics [scale=0.60] {my_folder/images/ch4/simple_reference.png}
    \caption{Каждый агент наблюдает целевой ориентир другого и производит коммуникационное действие, которое будет получено другим агентом на следующем шаге}
    \label{fig:exp-sr}
\end{figure}

В этом сценарии был поставлен эксперимент с декомпозированным вознаграждением, а также эксперимент с общим мозгом

Так же на этом сценарии производилось измерение консистентности коммуникационных действий.

\subsection{Сценарий 3. Simple World Communication} \label{exp-swc}

Кратко этот сценарий был описан в разделе \hyperref[intro-swc]{Вводная глава: Сценарий 3. Simple World Communication}. На \firef{fig:swc} показан сценарий игры и поведение агентов при использовании оптимальных политик. В этом сценарии лидер выступает говоруном, остальные преследователи – слушателями. И те, и другие перемещаются в погоне за хорошими агентами. Лидер выступает говоруном, а преследователи~--- слушателями. Высказывание лидера на одном шаге наблюдается другими преследователями на следующем шаге, как показано на \firef{fig:exp-swc}.

Хорошие агенты просто наблюдают еду и преследователей и перемещаются.

Под едой подразумевается объект, при приближении к которому, хорошие агенты получают награду

\begin{figure}[ht!]
    \center
    \includegraphics [scale=0.60] {my_folder/images/ch4/simple_world_communication.png}
    \caption{Лидер видит то, чего, возможно, не видят другие преследователи, и производит коммуникационное действие, которое будет получено преследователями на следующем шаге}
    \label{fig:exp-swc}
\end{figure}

Каждый преследователь наблюдает скорость $\upsilon \in \mathbb{R}^2$, расстояние до жертв $p = [p_1, p_2, p_3], p_i \in \mathbb{R}$, и высказывание лидера. Лидер наблюдает то же самое, кроме высказывания. Действие лидера состоит из двух поддействий: физического движения $u$ и коммуникационного действия $c$. Действие преследователя – только из физического движения $u$.
Наблюдение и действие лидера:

\begin{equation}
    \begin{multlined}
        o_i = \begin{bmatrix}
                  \upsilon \\ p
        \end{bmatrix}, \\
        a_i = \begin{bmatrix}
                  u \\ c
        \end{bmatrix}.
    \end{multlined}
\end{equation}

Наблюдение и действие преследователя:

\begin{equation}
    \begin{multlined}
        o_i = \begin{bmatrix}
                  \upsilon \\ p \\ c
        \end{bmatrix}, \\
        a_i = \begin{bmatrix}
                  u
        \end{bmatrix}.
    \end{multlined}
\end{equation}

Это показано на рисунке \firef{fig:exp-swc}.

Наградой каждого преследователя является отрицательное евклидово расстояние до ближайшего агента.

Во время обучения лидер учится, как закодировать положение жертвы, чтобы передать его преследователю. И преследователь должен изучить закодированные высказывания лидера и двигаться к жертве.

Архитектура нейронных сетей в этом сценарии аналогична архитектуре в предыдущем сценарии.

На этом сценарии производилось измерение консистентности коммуникационных действий.

\subsection{Сценарий 4: Simple Tag} \label{exp-st}

Кратко этот сценарий был описан в разделе \hyperref[intro-st]{Вводная глава: Сценарий 4. Simple Tag}. На \firef{fig:st} показан сценарий игры и поведение агентов при использовании оптимальных политик. В этом сценарии нет общения между агентами, но пространства наблюдения и действий всех преследователей совпадают, это позволяет применить к нему метод обучения с одним мозгом. В этом сценарии все агенты наблюдают друг друга и перемещаются по игровому полю.

Каждый преследователь наблюдает скорость $\upsilon \in \mathbb{R}^2$, расстояние до жертв ${p = [p_1, p_2, p_3], p_i \in \mathbb{R}}$.

Наблюдение и действие:

\begin{equation}
    \begin{multlined}
        o_i = \begin{bmatrix}
                  \upsilon \\ p
        \end{bmatrix}, \\
        a_i = \begin{bmatrix}
                  u
        \end{bmatrix}.
    \end{multlined}
\end{equation}

Наградой каждого преследователя является отрицательное евклидово расстояние до ближайшего агента.

Во время обучения преследователи учатся догонять жертву, а жертва – убегать.

Архитектура нейронных сетей в этом сценарии аналогична архитектуре в предыдущем сценарии.

На этом сценарии ставился эксперимент с одним мозгом.


\section{Сетевая архитектура} \label{network-architecture}

В таблице \taref{tab-algs-application} показана примененимость различных сетевых архитектур к различным игровым сценариям.

\begin{table}[t!]
    \centering\small
    \caption{Подходящие сетевые архитектуры для разных игровых сценариев}
    \label{tab-algs-application}
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        & MADDPG & MADDPG с одним мозгом & Декомпозированная награда \\
        \hline
        Speaker Listener    & v      & ---                   & ---                       \\ \hline
        Simple Reference    & v      & v                     & v                         \\ \hline
        World Communication & v      & ---                   & v                         \\ \hline
        Simple Tag          & v      & v                     & ---                       \\ \hline
    \end{tabular}
    \normalsize% возвращаем шрифт к нормальному
\end{table}

Такие сценарии как Simple Speaker Listener с двумя агентами, разделяющими глобальное вознаграждение, но обладающими разными наблюдениями и пространствами действий, могут использовать только стандартный MADDPG. Поскольку агенты функционируют по-разному, им необходимо искать различные оптимальные политики для своих ролей в игре.

Такие сценарии как Simple Reference, где два агента совместно получают глобальное вознаграждение, имеют одно и то же пространство действий и пространство наблюдений, могут работать со стандартным MADDPG, а также эти сценарии могут использовать общий мозг или декомпозированную награду.

Наиболее эффективной архитектурой являются использование сети с общим мозгом, поскольку при этом обучается меньшее количество сетей. Это значительно повышает эффективность обучения игровых сценариев, которые можно адаптировать к сетям с общим мозгом.

В сценарии Simple Tag агенты из одной команды также имеют одинаковые пространства наблюдений и действий. Здесь был применён отдельный набор акторов-критиков для преследователей и отдельный - для жертв. Общение в этом сценарии отсутствует, поэтому декомпозированная награда здесь не применима. Этот сценарий был выбран для сравнения стандартного MADDPG и MADDPG с одним мозгом.

В экспериментах со сценариями, в которых агенты общаются между собой (см. \taref{tab-communicational-scenarious}), производилось измерение консистентности коммуникационных действий.

\begin{table}[t!]
    \centering\small
    \caption{Наличие действий общения в сценариях}
    \label{tab-communicational-scenarious}
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        & Предполагает общение агентов \\
        \hline
        Speaker Listener    & v                            \\
        \hline
        Simple Reference    & v                            \\
        \hline
        World Communication & v                            \\
        \hline
        Simple Tag          & ---                          \\
        \hline
    \end{tabular}
    \normalsize% возвращаем шрифт к нормальному
\end{table}

Сценарии, в которых одни и те же агенты и перемещаются и говорят, подходят для применения декомпозированой награды. Simple Listener не подходит, т.к. один агент только перемещается, а другой только говорит, а в Simple Tag нет действий общения.

Таким образом, для ответа на \hyperref[intro-questions]{вопросы исследования} ставились эксперименты, показывающие общение агентов между собой, а также эксперименты, показывающие эффективность некоторых модификаций алгоритма, направленных на ускорение обучения. Исходя из этого и выбирались сценарии для экспериментов данной работы.
