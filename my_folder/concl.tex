\chapter*{Заключение} \label{ch-conclusion}
\addcontentsline{toc}{chapter}{Заключение}    % в оглавление

Данная работа способствует дальнейшему пониманию совместной работы множества агентов в игровой среде. Проведенные эксперименты доказывают, что агенты должны вырабатывать оптимальную политику сотрудничества с учетом полного наблюдения за состоянием окружающей среды и политиками других агентов. Очевидно, что в результате обучения возникает композиционный язык, который улучшает сотрудничество. К сожалению, в данной работе алгоритм работает в определённых сценариях, но не удалось реализовать все задуманные приемы, и не все реализованные варианты алгоритма удалось заставить работать, вероятно, из-за нехватки времени и ресурсов.

По сравнению с предыдущими работами эта дипломная работа расширяет MADDPG до более сложного сценария, когда агентам необходимо сотрудничать в многомерных пространствах действий. Вкратце, агенты одновременно выполняют физическое движение и коммуникационное высказывание.

Некоторые варианты MADDPG исследуются для разных игровых сценариев.

MADDPG с разложенным вознаграждением может быть адаптирован к сценариям со сложным глобальным вознаграждением.

MADDPG с одним мозгом может применяться, если агенты в сценарии симметричны в пространстве наблюдения и действия и имеют одинаковое глобальное вознаграждение. Кроме того, агенты, обученные MADDPG с одним мозгом, говорят на одном языке, что важно для сценариев, с более чем двумя агентами, которые должны общаться между собой для достижения цели.

Проблема насыщения сети постоянно встречается в процессе дипломной работы. Возможным объяснением этого может быть то, что для изучения оптимальных политик в пространстве непрерывных действий нейронные сети должны быть достаточно сложными, чтобы извлекать скрытые функции, оценивать Q значения или оптимизировать политики.

\paragraph{Дальнейшая работа}
Естественным продолжением дальнейшего изучения мультиагентного взаимодействия было бы исследование сотрудничества большего количества агентов. Они могут обладать более многомерным пространством действий. Они должны говорить на одном языке, если необходимо общение между более чем двумя агентами. Следовательно, алгоритмы и сети необходимо модифицировать и адаптировать с учетом новых ситуаций. Приведенные выше выводы в этой дипломной работе могут способствовать дальнейшим исследованиям.

Кроме того, все ещё стоит дополнительно изучить использование алгоритма детерминированного градиента политики для задач с несколькими агентами в сценариях трехмерных игр. Основной проблемой для алгоритма в сценариях 3D-игр может быть высокоразмерный ввод пикселей. Дальнейшая работа должна быть сделана соответственно над структурой нейронных сетей, например, сверточных слоев в актере и настройке гиперпараметров.

После отработки алгоритма на 3D средах с высокоразмерным вводом, можно пробовать применить алгоритм к обучению нескольких роботов для достижения общей цели в реальной среде. Например, сбор мусора, различные задачи на преследование и т. д.

TODO: Последним абзацем в заключении можно выразить благодарность всем людям, которые помогали автору в написании ВКР.