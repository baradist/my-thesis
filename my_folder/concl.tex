\chapter*{Заключение} \label{ch-conclusion}
\addcontentsline{toc}{chapter}{Заключение}    % в оглавление

Как это было сказано во \hyperref[intro]{введении}, целью данной работы является разработка и исследование алгоритмов управления МАС с применением технологий глубокого обучения с подкреплением. 

Во время работы над этой выпускной работой были решены поставленные задачи: был произведён обзор технологии глубокого обучения с подкреплением, а так же методов и алгоритмов управления МАС; были поставлены вопросы исследования, а затем выбраны и доработаны алгоритмы и игровые сценарии, которые позволяют дать ответы на эти вопросы; поставлены эксперименты и сделаны выводы.

В ходе исследования мы пришли к выводу, что лучше всего для управления в основном алгоритм \hyperref[acr:maddpg]{MADDPG}. Сам алгоритм хорошо описан \cite{lowe2017multiagent}. Мы же решили исследовать, могут ли агенты сотрудничать, а так же, общение агентов между собой. Как упоминалось \hyperref[ch2:ma-algs]{выше}, многие задачи из реального мира лучше решаются многоагентными алгоритмами, для достижения оптимального результата, действия агентов должны быть скоординированы.

С другой стороны, чем больше агентов и чем сложнее среда, тем больше ресурсов требуется на обучение. Поэтому следующий вопрос был посвящен методам оптимизации обучения.

Данная работа способствует дальнейшему пониманию совместной работы множества агентов в игровой среде. Проведенные эксперименты доказывают, что агенты должны вырабатывать оптимальную политику сотрудничества с учетом полного наблюдения за состоянием окружающей среды и политиками других агентов. Очевидно, что в результате обучения возникает композиционный язык, который улучшает сотрудничество. К сожалению, в данной работе алгоритм работает в определённых сценариях, но не удалось реализовать все задуманные приемы, и не все реализованные варианты алгоритма удалось заставить работать, вероятно, из-за нехватки времени и ресурсов.

По сравнению с предыдущими работами эта дипломная работа расширяет MADDPG до более сложного сценария, когда агентам необходимо сотрудничать в многомерных пространствах действий. Вкратце, агенты одновременно выполняют физическое движение и коммуникационное высказывание.

Некоторые варианты MADDPG исследуются для разных игровых сценариев.

MADDPG с разложенным вознаграждением может быть адаптирован к сценариям со сложным глобальным вознаграждением.

MADDPG с одним мозгом может применяться, если агенты в сценарии симметричны в пространстве наблюдения и действий и имеют одинаковое глобальное вознаграждение. Кроме того, агенты, обученные MADDPG с одним мозгом, говорят на одном языке, что важно для сценариев, с более чем двумя агентами, которые должны общаться между собой для достижения цели.

\paragraph{Дальнейшая работа}
Естественным продолжением дальнейшего изучения мультиагентного взаимодействия было бы исследование сотрудничества большего количества агентов. Они могут обладать более многомерным пространством действий. Они должны говорить на одном языке, если необходимо общение между более чем двумя агентами. Следовательно, алгоритмы и сети необходимо модифицировать и адаптировать с учетом новых ситуаций. Приведенные выше выводы в этой дипломной работе могут способствовать дальнейшим исследованиям.

Алгоритм MADDPG считается довольно капризным и требующим тонкой настройки. Возможно поэтому, а так же, из-за недостатка времени и вычислительных ресурсов, не увенчались успехом эксперименты по некоторым методам оптимизации обучения: \textit{декомпозированное вознаграждение} и \textit{обучение по учебной программе}. Следующими шагами в данном исследовании могли бы быть связаны с тем, чтобы всётаки реализовать эти методы и поставить соответствующие эксперименты.

Кроме того, все ещё стоит дополнительно изучить использование алгоритма \hyperref[acr:dpg]{детерминированного градиента политики} для задач с несколькими агентами в сценариях трехмерных игр. Основной проблемой для алгоритма в сценариях 3D-игр может быть высокоразмерный ввод пикселей. Дальнейшая работа должна быть сделана соответственно над структурой нейронных сетей, например, сверточных слоев в акторе и настройке гиперпараметров.

После отработки алгоритма на 3D средах с высокоразмерным вводом, можно пробовать применить алгоритм к обучению нескольких роботов для достижения общей цели в реальной среде. Например, сбор мусора, различные задачи на преследование и т. д.

TODO: Последним абзацем в заключении можно выразить благодарность всем людям, которые помогали автору в написании ВКР.