\chapter*{Заключение} \label{ch-conclusion}
\addcontentsline{toc}{chapter}{Заключение}    % в оглавление

Как это было сказано во \hyperref[intro]{введении}, целью данной работы является разработка и исследование алгоритмов управления МАС с применением технологий глубокого обучения с подкреплением. 

Во время работы над этой выпускной работой были решены поставленные задачи: был произведён обзор технологии глубокого обучения с подкреплением, а также методов и алгоритмов управления МАС; были поставлены вопросы исследования, а затем выбраны и доработаны алгоритмы и игровые сценарии, которые позволяют дать ответы на эти вопросы; поставлены эксперименты и сделаны выводы.

В ходе исследования был сделан вывод, что лучше всего для управления в мультиагентной среде подходит алгоритм \hyperref[acr:maddpg]{MADDPG}. Сам алгоритм хорошо описан \cite{lowe2017multiagent}. В этой же работе было решено исследовать, могут ли агенты сотрудничать, а также общение агентов между собой. Как упоминалось в разделе \hyperref[ch2:ma-algs]{3.2 Мультиагентные алгоритмы}, многие задачи из реального мира лучше решаются многоагентными алгоритмами; для достижения оптимального результата действия агентов должны быть скоординированы.

С другой стороны, чем больше агентов и чем сложнее среда, тем больше ресурсов требуется на обучение. Поэтому следующий вопрос был посвящён методам оптимизации обучения.

Данная работа способствует дальнейшему пониманию совместной работы множества агентов в игровой среде. Проведённые эксперименты доказывают, что агенты должны вырабатывать оптимальную политику сотрудничества с учётом полного наблюдения за состоянием окружающей среды и политиками других агентов. Очевидно, что в результате обучения возникает композиционный язык, который улучшает сотрудничество. В данной работе алгоритм действует в определённых сценариях, но к сожалению, не удалось реализовать все задуманные приёмы, и не все реализованные варианты алгоритма удалось заставить работать, вероятно, из-за нехватки времени и ресурсов.

По сравнению с предыдущими работами эта дипломная работа расширяет MADDPG до более сложного сценария, когда агентам необходимо сотрудничать в многомерных пространствах действий. Вкратце, агенты одновременно выполняют физическое движение и коммуникационное высказывание.

Некоторые варианты MADDPG исследуются для разных игровых сценариев.

MADDPG с разложенным вознаграждением может быть адаптирован к сценариям со сложным глобальным вознаграждением.

MADDPG с \textit{одним мозгом} может применяться, если агенты в сценарии симметричны в пространстве наблюдения и действий и имеют одинаковое глобальное вознаграждение. Кроме того, агенты, обученные MADDPG с \textit{одним мозгом}, говорят на одном языке, что важно для сценариев с более чем двумя агентами, которые должны общаться между собой для достижения общей цели.

Как уже упоминалось \hyperref[exp-results-svc]{выше}, из-за конкурентной природы сценариев \textit{Simple World Communication} и \textit{Simple Tag} нам не удалось добиться того, чтобы награда агентов из разных команд стремилась к какому-то значению или постоянно росла. Эти сценарии были выбраны для того, чтобы исследовать вопросы, поставленные нами в \hyperref[intro-questions]{начале данной работы}.

Зато эксперименты с этими сценариями показали эффективность алгоритма MADDPG \textit{с общим мозгом}.

Опишем возможную \textbf{дальнейшую работу} в продолжение данного исследования.

Алгоритм MADDPG считается довольно капризным и требующим тонкой настройки. Возможно поэтому, а также из-за недостатка времени и вычислительных ресурсов, не увенчались успехом эксперименты по некоторым методам оптимизации обучения: \textit{декомпозированное вознаграждение} и \textit{обучение по учебной программе}.

\textbf{Следующие шаги} в данном исследовании могут быть связаны с тем, чтобы всё-таки реализовать эти методы и поставить соответствующие эксперименты.

Затем следует исследовать сотрудничество большего количества агентов. Они могут обладать более многомерным пространством действий. Агенты должны говорить на одном языке, если необходимо общение между более чем двумя агентами. Следовательно, алгоритмы и сети необходимо модифицировать и адаптировать с учётом новых ситуаций. Приведённые выше выводы в этой дипломной работе могут способствовать дальнейшим исследованиям.

Следующее усложнение - это использование алгоритма \hyperref[acr:dpg]{детерминированного градиента политики} для задач с несколькими агентами в сценариях трёхмерных игр. Основной проблемой для алгоритма в таких сценариях может быть высокоразмерный ввод пикселей. Дальнейшая работа должна быть сделана соответственно над структурой нейронных сетей, например, сверточных слоёв в \textit{акторе} и настройке гиперпараметров.

Все эти этапы - это шаги с постепенным усложнением задач перед попыткой применения уже отработанных алгоритмов к управлению роботами и их совместной командной работе.

Основные сложности применения алгоритмов \hyperref[acr:rl]{обучения с подкреплением} к управлению роботами заключаются в следующем:
\begin{itemize}
	\item очень высокая размерность наблюдения;
	\item высокая степень неопределённости (никогда нельзя быть до конца уверенным, как поменяется среда после очередного действия);
	\item дороговизна и длительность тренировки модели в реальной среде (невозможно сократить время тренировки за счёт вычислительных мощностей), а зачастую невозможность такой тренировки (в процессе обучения неизбежны ошибки; например автомобиль, управляемый программой, может представлять опасность).
\end{itemize}

Мы считаем, что из всего вышесказанного следует, что первые этапы изучения следует производить в симуляторах естественной среды.

И наконец, следующим шагом была бы апробация применения алгоритм к обучению нескольких роботов для достижения общей цели в реальной среде. Например, сбор мусора, различные задачи на преследование и т.д.

Напоследок, хотелось бы выразить бладодарность за помощь в написании данной работы моему научному руководителю, доценту ВШиСТ Паку Вадиму Геннадьевичу, а также научному сотруднику СПИИРАН Клеверову Денису Анатольевичу за помощь в технических вопросах и за рецензию. Без этих людей этой работы не было бы.
