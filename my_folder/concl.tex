\chapter*{Заключение} \label{ch-conclusion}
\addcontentsline{toc}{chapter}{Заключение}    % в оглавление

Как это было сказано во \hyperref[intro]{введении}, целью данной работы является разработка и исследование алгоритмов управления МАС с применением технологий глубокого обучения с подкреплением. 

Во время работы над этой выпускной работой были решены поставленные задачи: был произведён обзор технологии глубокого обучения с подкреплением, а также методов и алгоритмов управления МАС; были поставлены вопросы исследования, а затем выбраны и доработаны алгоритмы и игровые сценарии, которые позволяют дать ответы на эти вопросы; поставлены эксперименты и сделаны выводы.

В ходе исследования мы пришли к выводу, что лучше всего для управления в основном алгоритм \hyperref[acr:maddpg]{MADDPG}. Сам алгоритм хорошо описан \cite{lowe2017multiagent}. Мы же решили исследовать, могут ли агенты сотрудничать, а также общение агентов между собой. Как упоминалось \hyperref[ch2:ma-algs]{выше}, многие задачи из реального мира лучше решаются многоагентными алгоритмами, для достижения оптимального результата, действия агентов должны быть скоординированы.

С другой стороны, чем больше агентов и чем сложнее среда, тем больше ресурсов требуется на обучение. Поэтому следующий вопрос был посвящен методам оптимизации обучения.

Данная работа способствует дальнейшему пониманию совместной работы множества агентов в игровой среде. Проведенные эксперименты доказывают, что агенты должны вырабатывать оптимальную политику сотрудничества с учетом полного наблюдения за состоянием окружающей среды и политиками других агентов. Очевидно, что в результате обучения возникает композиционный язык, который улучшает сотрудничество. К сожалению, в данной работе алгоритм работает в определённых сценариях, но не удалось реализовать все задуманные приемы, и не все реализованные варианты алгоритма удалось заставить работать, вероятно, из-за нехватки времени и ресурсов.

По сравнению с предыдущими работами эта дипломная работа расширяет MADDPG до более сложного сценария, когда агентам необходимо сотрудничать в многомерных пространствах действий. Вкратце, агенты одновременно выполняют физическое движение и коммуникационное высказывание.

Некоторые варианты MADDPG исследуются для разных игровых сценариев.

MADDPG с разложенным вознаграждением может быть адаптирован к сценариям со сложным глобальным вознаграждением.

MADDPG с \textit{одним мозгом} может применяться, если агенты в сценарии симметричны в пространстве наблюдения и действий и имеют одинаковое глобальное вознаграждение. Кроме того, агенты, обученные MADDPG с \textit{одним мозгом}, говорят на одном языке, что важно для сценариев, с более чем двумя агентами, которые должны общаться между собой для достижения общей цели.

Как уже упоминалось \hyperref[exp-results-svc]{выше}, из-за конкурентной природы сценариев \textit{Simple World Communication} и \textit{Simple Tag}, нам не удалось добиться того, чтобы награда агентов из разных команд стремилась к какому-то значению или постоянно росла. Мы выбрали эти сценарии для того, чтобы исследовать вопросы поставленные нами в \hyperref[intro-questions]{начале данной работы}.

Зато эксперименты с этими сценариями показали эффективность алгоритма MADDPG \textit{с общим мозгом}.

Опишем возможную \textbf{дальнейшую работу} в продолжение данного исследования.

Алгоритм MADDPG считается довольно капризным и требующим тонкой настройки. Возможно поэтому, а также из-за недостатка времени и вычислительных ресурсов, не увенчались успехом эксперименты по некоторым методам оптимизации обучения: \textit{декомпозированное вознаграждение} и \textit{обучение по учебной программе}.

\textbf{Следующие шаги} в данном исследовании могут быть связаны с тем, чтобы всё-таки реализовать эти методы и поставить соответствующие эксперименты.

Затем следует исследовать сотрудничество большего количества агентов. Они могут обладать более многомерным пространством действий. Агенты должны говорить на одном языке, если необходимо общение между более чем двумя агентами. Следовательно, алгоритмы и сети необходимо модифицировать и адаптировать с учетом новых ситуаций. Приведенные выше выводы в этой дипломной работе могут способствовать дальнейшим исследованиям.

Следующее усложнение - это использование алгоритма \hyperref[acr:dpg]{детерминированного градиента политики} для задач с несколькими агентами в сценариях трехмерных игр. Основной проблемой для алгоритма в сценариях 3D-игр может быть высокоразмерный ввод пикселей. Дальнейшая работа должна быть сделана соответственно над структурой нейронных сетей, например, сверточных слоев в \textit{акторе} и настройке гиперпараметров.

Все эти этапы - это шаги с постепенным усложнением задач, перед попыткой применения уже отработанных алгоритмов к управлению роботами и их совместной командной работе. 

Основные сложности применения алгоритмов \hyperref[acr:rl]{обучения с подкреплением} к управлению роботами, заключаются в:
\begin{itemize}
	\item очень высокая размерность наблюдения;
	\item высокая степень неопределённости (мы никогда до конца не знаем, как поменяется среда после очередного нашего действия);
	\item очень тренировать модель в реальной среде очень дорого, долго (мы не можем ускорить течение времени за счёт вычислительных мощьностей), а часто и вовсе невозможно, поскольку в процессе обучения неизбежны ошибки, и, например автомобиль, управляемый программой, может представлять опасность.
\end{itemize}

Мы считаем, что из всего вышесказанного следует, что первые этапы изучения следует производить в симуляторах естесственной среды.

И наконец, следующим шагом было бы пробовать применить алгоритм к обучению нескольких роботов для достижения общей цели в реальной среде. Например, сбор мусора, различные задачи на преследование и т. д.

TODO: Последним абзацем в заключении можно выразить благодарность всем людям, которые помогали автору в написании ВКР.