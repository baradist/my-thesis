\chapter{Результаты и обсуждения}


\section{Ответы на вопросы исследования}

Мы получили следующие ответы на вопросы исследования, поставленные в разделе \hyperref[intro-questions]{Вопросы исследования}.

1. Как несколько агентов могут научиться сотрудничать друг с другом во время обучения в определенных игровых сценариях?

Ответ: во время обучения, для изучения оптимальной политики сотрудничества, агенты должны принимать во внимание совместные наблюдения и совместные действия всех агентов.

2. Может ли после обучения появиться язык между агентами в определенных игровых сценариях?

Ответ: да, когда обучение сходится, композиционный язык у агентов возникает.

3. Как можно оптимизировать и ускорить процесс обучения?

Ответ: для ускорения обучения существуют разные методы и приемы. В этой работе мы пробовали применить некоторые из них, такие как: обучение с общим мозгом, декомпозированное вознаграждение, обучение по учебной программе.

\newpage


\section{Сетевая архитектура}

В таблице \taref{tab-algs-application} показано применение различных сетевых архитектур к различным игровым сценариям.

\begin{table}[t!]
    \centering\small
    \caption{Подходящие сетевые архитектуры для разных игровых сценариев}
    \label{tab-algs-application}
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        & MADDPG & MADDPG с одним мозгом & Декомпозированная награда \\
        \hline
        Speaker Listener    & v      & x                     & x                         \\ \hline
        Simple Reference    & v      & v                     & v                         \\ \hline
        World Communication & v      & x                     & v                         \\ \hline
        Simple Tag          & v      & v                     & x                         \\ \hline
    \end{tabular}
    \normalsize% возвращаем шрифт к нормальному
\end{table}

Такие сценарии, как \textit{Simple Speaker Listener} с двумя агентами, разделяющими глобальное вознаграждение, но обладающими разными наблюдениями и пространствами действий, могут использовать только стандартный MADDPG. Поскольку агенты функционируют по-разному, им необходимо искать различные оптимальные политики для своих ролей в игре.

Такие сценарии, как \textit{Simple Reference}, где два агента совместно получают глобальное вознаграждение, имеют одно и то же пространство действий и пространство наблюдений могут работать со стандартным MADDPG, а также эти сценарии могут использовать \textit{общий мозг}, или \textit{декомпозированную награду}.

Наиболее эффективной архитектурой являются использование сети с \textit{общим мозгом}, поскольку при этом обучается меньшее количество сетей. Это значительно повышает эффективность обучения игровых сценариев, которые можно адаптировать к сетям с \textit{общим мозгом}.

В сценарии \textit{Simple Tag} агенты из одной «команды» так же имеют одинаковые пространства наблюдений и действий. Мы применили здесь отдельный набор \textit{акторов-критиков} для \textit{преследователей} и отдельный - для \textit{жертв}. Общение в этом сценарии отсутствует, поэтому \textit{декомпозированная награда} здесь не имеет смысла. Этот сценарий был выбран для сравнения стандартного MADDPG с MADDPG с \textit{одним мозгом}.

\newpage


\section{Сценарии}

Первые два сценария не предполагают конкуренции и не требуют большого количества шагов для достижения результата, поэтому их эпизоды ограничены 25 шагами. 

Эксперименты со вторыми двумя сценариями мы решили дать агентам возможность двигаться подольше и ограничили их длительность 150 шагами. Это замедлило обучение, но иначе невозможно обучить агентам сколько-нибудь сложномым тактикам.

Во всех экспериментах проигрывалось 20000 эпизодов.

\input{my_folder/ch5/scenario-ssl}
\input{my_folder/ch5/scenario-sr}
\input{my_folder/ch5/scenario-swc}
\input{my_folder/ch5/scenario-st}


\section{Выводы и замечания}

Алгоритм MADDPG считается довольно капризным и требует тонкой настройки. Во время проведения экспериментов алгоритм сходился далеко не всегда, а некоторые эксперименты так и не увенчались успехом.

Обучение сходится с алгоритмом MADDPG, а также с вариантом с \textit{одним мозгом}. К сожалению, при выполнении эксперимента с \textit{учебной программой} не удается применить модель, обученную на простых настройках к более сложным. Агенты, хорошо выступающие на простом уровне, расходятся после повышения уровня сложности. Причиной является то, что при усложнении настроек среды, растет и измерение наблюдений. А так же, возможно, то, что обучение модели происходит не сразу, а после наполнения буфера.

Эксперимент с \textit{декомпозированной наградой} тоже не увенчался успехом, вероятно, потому что награда поступает из среды за общее действие, а не за отдельные поддействия. Возможно, если обучать модель значительно дольше, мы увидели бы более хорошие результаты.


\newpage
