\chapter{Результаты и обсуждения}

\section{Ответы на вопросы исследования}

Мы получили следующие ответы на вопросы исследования, поставленные в разделе \hyperref[intro-questions]{Вопросы исследования}.

1.	Как несколько агентов могут научиться сотрудничать друг с другом во время обучения в определенных игровых сценариях?

Ответ: во время обучения, для изучения оптимальной политики сотрудничества, агенты должны принимать во внимание совместные наблюдения и совместные действия всех агентов.

2 Может ли после обучения появиться язык между агентами в определенных игровых сценариях?

Ответ: да, когда обучение сходится, композиционный язык у агентов возникает.

3.	Как можно оптимизировать и ускорить процесс обучения?

Ответ: для ускорения обучения существуют разные методы и приемы. В этой работе мы пробовали применить некоторые из них, такие как: обучение с общим мозгом, декомпозированное вознаграждение, обучение по учебной программе.

\newpage

\section{Сетевая архитектура}

В таблице \taref{tab-algs-application} показано применение различных сетевых архитектур к различным игровым сценариям.

\begin{table}[t!]
	\centering\small
	\caption{Подходящие сетевые архитектуры для разных игровых сценариев.}%
	\label{tab-algs-application}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		&MADDPG&MADDPG с одним мозгом&Декомпозированная награда\\
		\hline
		Speaker Listener&v&x&x\\ \hline
		Simple Reference&v&v&v\\ \hline
		World Communication&v&x&v\\ \hline
		Simple Tag&v&v&x\\ \hline
	\end{tabular}
	\normalsize% возвращаем шрифт к нормальному
\end{table}

Такие сценарии, как \textit{Simple Speaker Listener} с двумя агентами, разделяющими глобальное вознаграждение, но обладающими разными наблюдениями и пространствами действий, могут использовать только стандартный MADDPG. Поскольку агенты функционируют по-разному, им необходимо искать различные оптимальные политики для своих ролей в игре.

Такие сценарии, как \textit{Simple Reference}, где два агента совместно получают глобальное вознаграждение, имеют одно и то же пространство действий и пространство наблюдений могут работать со стандартным MADDPG, а также эти сценарии могут использовать \textit{общий мозг}, или \textit{декомпозированную награду}.

Наиболее эффективной архитектурой являются использование сети с \textit{общим мозгом}, поскольку при этом обучается меньшее количество сетей. Это значительно повышает эффективность обучения игровых сценариев, которые можно адаптировать к сетям с \textit{общим мозгом}. 

В сценарии \textit{Simple Tag} агенты из одной «команды» так же имеют одинаковые пространства наблюдений и действий. Мы применили здесь отдельный набор \textit{акторов-критиков} для \textit{преследователей} и отдельный - для \textit{жертв}. Общение в этом сценарии отсутствует, поэтому \textit{декомпозированная награда} здесь не имеет смысла. Этот сценарий был выбран для сравнения стандартного MADDPG с MADDPG с \textit{одним мозгом}.

\newpage

\section{Сценарии}

\input{my_folder/ch5/scenario-ssl}
\input{my_folder/ch5/scenario-sr}
\input{my_folder/ch5/scenario-swc}
\input{my_folder/ch5/scenario-st}

\newpage
