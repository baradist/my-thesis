\chapter{Результаты и обсуждения}


\section{Ответы на вопросы исследования}

Были получены ответы на вопросы исследования, поставленные в разделе \hyperref[intro-questions]{Вопросы исследования}.

1. Как несколько агентов могут научиться сотрудничать друг с другом во время обучения в определённых игровых сценариях?

Ответ: во время обучения, для изучения оптимальной политики сотрудничества, агенты должны принимать во внимание совместные наблюдения и совместные действия всех агентов.

2. Может ли после обучения появиться язык между агентами в определённых игровых сценариях?

Ответ: да, когда обучение сходится, композиционный язык у агентов возникает.

3. Как можно оптимизировать и ускорить процесс обучения?

Ответ: для ускорения обучения существуют разные методы и приёмы. В этой работе применялись некоторые из них, такие как: обучение с общим мозгом, декомпозированное вознаграждение, обучение по учебной программе.

\newpage

\section{Сценарии}

Первые два сценария не предполагают конкуренции и не требуют большого количества шагов для достижения результата, поэтому их эпизоды ограничены 25 шагами.

В экспериментах со вторыми двумя сценариями было решено дать агентам возможность подвигаться подольше и длительность эпизодов была ограничена 150 шагами. Это замедлило обучение, но иначе невозможно обучить агентам сколько-нибудь сложномым тактикам.

Во всех экспериментах проигрывалось 20000 эпизодов.

\input{my_folder/ch5/scenario-ssl}
\input{my_folder/ch5/scenario-sr}
\input{my_folder/ch5/scenario-swc}
\input{my_folder/ch5/scenario-st}


\section{Выводы и замечания}

Алгоритм MADDPG считается довольно капризным и требует тонкой настройки. Во время проведения экспериментов алгоритм сходился далеко не всегда, а некоторые эксперименты так и не увенчались успехом.

Обучение сходится с алгоритмом MADDPG, а также с вариантом с \textit{одним мозгом}. К сожалению, при выполнении эксперимента с \textit{учебной программой} не удаётся применить модель, обученную на простых настройках к более сложным. Агенты, хорошо выступающие на простом уровне, расходятся после повышения уровня сложности. Причиной является то, что при усложнении настроек среды, растёт и измерение наблюдений. А также, возможно, то, что обучение модели происходит не сразу, а после наполнения буфера.

Эксперимент с \textit{декомпозированной наградой} тоже не увенчался успехом, вероятно, потому что награда поступает из среды за общее действие, а не за отдельные поддействия. Возможно, если обучать модель значительно дольше, можно было бы увидеть более хорошие результаты.


\newpage
