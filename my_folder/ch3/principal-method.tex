\section{Основные методы}

\subsection{Архитектура актор-критик}

Как уже упоминалось выше, алгоритм MADDPG имеет набор сетей актор-критиков для каждого агента в игровой среде. Для облегчения многоагентной совместной работы, во время тренировки, алгоритм учитывает наблюдения и действия всех агентов. Политики оптимизируются путем оценки качества, поведения всех агентов в разных состояниях среды. Таким образом, в условиях среды, требующей взаимодействия агентов, будет разработана политика, оптимальная для сотрудничества.

Глубокая нейронная сеть может рассматриваться как аппроксиматор нелинейных функций. В решении сложных задач глубокого обучения с подкреплением нейронная сеть нестабильна \cite{lillicrap2015continuous}. Эту проблему решает использование целевой (target) сети, поскольку мягкое и разреженное обновление целевой сети замедляет ее приближение к исходной сети и уменьшает влияние ошибок. Таким образом, это нарушает корреляцию между текущими и целевым Q-значениями. Хотя это снижает скорость обучения, но и улучшает стабильность обучения. В алгоритме MADDPG и у актера, и у критика есть свои целевые сети.

\subsection{Воспроизведение опыта (Experience Replay)}

Воспроизведение опыта — это механизм, когда во время тренировки в игровой среде буфер используется для сбора опыта, образцы которого из этого буфера затем случайным образом извлекаются для обучения модели.

Опыт генерируется последовательно, во время взаимодействия агентов со средой в хронологическом порядке. Неизбежно то, что собранные последовательности опыта коррелируют друг с другом. Из-за этого сеть легко переобучается на имеющиеся последовательности опыта. В итоге, переобученная сеть не может обеспечить разнообразный опыт для последующего обучения.

Опыт случайно отбирается в небольшие блоки. Это нужно не только для эффективного использования аппаратного обеспечения и увеличения скорости обучения, но также нарушает корреляцию последовательности в буфере. Таким образом, сети имеют возможность формулировать более обобщенные политики по независимым друг от друга данным.

Для применения воспроизведение опыта, в буфер, складывается опыт в виде кортежей переходов, включающих наблюдения, действия, награды и последующие наблюдения, то есть $e_t = (\mathbf{s, a, r, s'})$. Поскольку это многоагентная игровая среда, \textbf{s} - это совместные наблюдения, которые представляют собой совокупность наблюдений n агентов, $\mathbf{s} = [s_0, s_1, ..., s_n]$. То же самое относится к действиям, следующим наблюдениям и наградам. Буфер имеет ограниченную емкость, и при переполнении старый опыт вытесняется новым.
