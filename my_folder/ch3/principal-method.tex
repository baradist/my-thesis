\section{Основные методы}

\subsection{Архитектура актор-критик}

Как уже упоминалось выше, алгоритм MADDPG имеет набор сетей акторов-критиков для каждого агента в игровой среде. Для облегчения многоагентной совместной работы во время тренировки алгоритм учитывает наблюдения и действия всех агентов. Политики оптимизируются путём оценки качества, поведения всех агентов в разных состояниях среды. Таким образом, в условиях среды, требующей взаимодействия агентов, будет разработана политика, оптимальная для сотрудничества.

Глубокая нейронная сеть может рассматриваться как аппроксиматор нелинейных функций. В решении сложных задач глубокого обучения с подкреплением нейронная сеть нестабильна \cite{lillicrap2015continuous}. Эту проблему решает использование целевой сети, поскольку мягкое и разрежённое обновление целевой сети замедляет её приближение к исходной сети и уменьшает влияние ошибок. Таким образом, нарушается корреляция между текущим и целевым Q-значениями. Хотя это и снижает скорость, но улучшает стабильность обучения. В алгоритме MADDPG и у актора, и у критика есть свои целевые сети.

\subsection{Воспроизведение опыта (Experience Replay)}

Воспроизведение опыта — это механизм, в котором во время тренировки в игровой среде буфер используется для сбора опыта, образцы которого из этого буфера затем случайным образом извлекаются для обучения модели.

Опыт генерируется последовательно во время взаимодействия агентов со средой в хронологическом порядке. Неизбежно то, что собранные последовательности опыта коррелируют друг с другом. Из-за этого сеть легко переобучается на имеющихся последовательностях опыта. В итоге, переобученная сеть не может обеспечить разнообразный опыт для последующего обучения.

Опыт случайно отбирается в небольшие блоки. Это нужно не только для эффективного использования аппаратного обеспечения и увеличения скорости обучения, но также нарушает корреляцию последовательности в буфере. Таким образом, сети имеют возможность формулировать более обобщённые политики по независимым друг от друга данным.

Для применения воспроизведения опыта в буфер складывается опыт в виде кортежей переходов, включающих наблюдения, действия, награды и последующие наблюдения, то есть ${e_t = (\mathbf{s, a, r, s'})}$. Поскольку это многоагентная игровая среда, \textbf{s}~--- это совместные наблюдения, которые представляют собой совокупность наблюдений n агентов, ${\mathbf{s} = [s_0, s_1, ..., s_n]}$. То же самое относится к действиям, следующим наблюдениям и наградам. Буфер имеет ограниченную ёмкость, и при переполнении старый опыт вытесняется новым.

\subsection{Тренировка ИНС}

В то время как агенты исследуют игровую среду, опыт перехода на каждом шаге собирается в буфер памяти. Обучение происходит только тогда, когда количество кортежей в буфере превышает определённую величину. Когда начинается обучение, актор и критик каждого из n агентов обновляется на каждом шаге, в то время как целевой актор и целевой критик обновляются с задержкой (раз в определённое количество шагов).

На рисунке \firef{fig:ch3-network-training} показано, как обновляется набор сетей акторов-критиков. Текущее Q-значение $Q_i(\mathbf{s, a}|\theta ^Q_i)$ оценивается по сети критика, при этом на вход подаются совместные текущие наблюдения \textbf{s} и совместные действия \textbf{a}. Целевое Q-значение $y_i$ рассчитывается из вознаграждения и дисконтированного следующего Q-значения $Q'_i(\mathbf{s', a'}|\theta ^{Q'_i})$, аппроксимированного по целевому критику. Входные данные для сети целевого критика – это совместные последующие наблюдения \textbf{s}' из буфера и совместные последующие действия \textbf{a}', выбранные целевыми сетями акторов:

\begin{figure}[ht!]
    \center
    \includegraphics [scale=0.5] {my_folder/images/ch3/maddpg-updating-networks.png}
    \caption{Обновление сети актора и критика для каждого агента в MADDPG}
    \label{fig:ch3-network-training}
\end{figure}

\begin{equation}
    \begin{multlined}
        \mathbf{a'} = [a'_0, a'_1, ..., a'_n] = \\
        = [\mu'_0(s'_0|\theta^{\mu'_0}), \mu'_1(s'_1|\theta^{\mu'_1}), ..., \mu'_n(s'_n|\theta^{\mu'_n})].
    \end{multlined}
\end{equation}

Целевое Q- значение:

\begin{equation}
    \begin{multlined}
        y_i = r_i + \gamma Q'_i(\mathbf{s', a'}|\theta^{Q'_i}).
    \end{multlined}
\end{equation}
где $r_i$ это награда – i-го агента, и $\gamma$~--- это коэффициент дисконтирования, или скорость затухания влияния будущих наград.

Наконец, сеть критика обновляется путём минимизации функции потерь:

\begin{equation}
    \begin{multlined}
        L_i(\theta^{Q_i}) = \frac{1}{S} \sum (y_i - Q_i(\mathbf{s, a}|\theta^{Q_i}))^2,
    \end{multlined}
\end{equation}
где $S$~--- это размер тренировочного набора.

Сеть актора i-го агента обновляется путём максимизации Q-значения сетью критика. Критик принимает совместные текущие наблюдения \textbf{s} из буфера и совместные текущие действия, в которых i-е действие подменяется последним действием, выбранным актором, то есть:

\begin{equation}
    \begin{multlined}
    [a_0, a_1, a_i, ..., a_n]
        = [a_0, a_1, \mu_i(s_i|\theta^{\mu_i}), ..., a_n].
    \end{multlined}
\end{equation}

Для оптимизации уместно преобразовать задачу максимизации в задачу минимизации. Следовательно, функция потерь для обновления сети актора:

\begin{equation}
    \begin{multlined}
        L_i(\theta^{\mu_i}) = -Q_i(\mathbf{s}, [a_0, a_1, \mu_i(s_i|\theta^{\mu_i}), ..., a_n]|\theta^{Q_i}).
    \end{multlined}
\end{equation}

Сети целевых актора и критика обновляются не полным копированием сетей актора и критика, а с использованием мягкого обновления (soft update):

\begin{equation}
    \begin{multlined}
        \theta^{\mu'_i} = \tau \theta^{\mu_i} + (1 - \tau)\theta^{\mu'_i},\\
        \theta^{Q'_i} = \tau \theta^{Q_i} + (1 - \tau)\theta^{Q'_i},
    \end{multlined}
\end{equation}
где $\tau$ обычно очень мала, например, 0,01 в этой работе.
