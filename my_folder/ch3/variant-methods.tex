\section{Варианты алгоритмов}

\subsection{MADDPG с декомпозированным вознаграждением (Decomposed Reward)} TODO: перевести

Предполагается сценарий игры, в котором агенты выполняют относительно независимые под-действия, каждое под-действие может воздействовать на среду и получать вознаграждение, которое отделено от глобального вознаграждения. Идея декомпозирования награды MADDPG в том, что для каждого под-действия может быть сформулирована независимая политика. Каждый агент, имеет \textit{n} независимых наборов сетей \textit{актор-критиков}, каждый из которых соответствует одному виду действия. Ожидается, что политики для видов действий могут быть оптимизированы путем обучения соответствующих групп критиков.

В этой работе планируется использовать этот вариант в сценарии \textit{Simple Reference}. Позже, в разделе TODO будет полее подробно описано, как декомпозируется глобальная награда и как формулируется оптимальная политика для каждого вида действия. % TODO

\subsection{MADDPG с общим мозгом} % TODO: перевести

Другой вариант MADDPG вдохновлен \cite{mordatch2017emergence}. В этом варианте есть только один набор сетей \textit{акторов-критиков}, который используется всеми агентами. Это подразумевает, что все агенты имеют одинаковую оптимальную политику. Этот подход использует предположение о том, что все агенты имеют одно и то же пространство действий и пространство наблюдений, и они также имеют общую глобальную награду. Этот подход особенно эффективен для коммуникационных действий, поскольку композиционный язык постоянно появляется среди всех участников игрового сценария. В варианте с общим набором сетей все агенты говорят на одном языке, в отличие от стандартного MADDPG, где каждый агент может интерпретировать один и тот же ориентир по-разному. Это особенно важно, когда сотрудничают более двух агентов, поскольку им нужно общаться на одном языке.
