\section{Варианты алгоритмов}

\subsection{MADDPG с декомпозированным вознаграждением (Decomposed Reward)}

Предполагается сценарий игры, в котором агенты выполняют относительно независимые поддействия, каждое поддействие может воздействовать на среду и приводить к вознаграждению, которое отделено от глобального вознаграждения. Идея декомпозирования награды MADDPG состоит в том, что для каждого поддействия может быть сформулирована независимая политика. Каждый агент имеет n независимых наборов сетей актор-критиков, каждый из которых соответствует одному виду действия. Ожидается, что политики для отдельных видов действий могут быть оптимизированы путём обучения соответствующих групп критиков.

В этой работе планируется использовать этот вариант в сценарии Simple Reference.

\subsection{MADDPG с общим мозгом}

Другой вариант MADDPG вдохновлён \cite{mordatch2017emergence}. В нём есть только один набор сетей акторов-критиков, который используется всеми агентами. Это подразумевает, что все агенты имеют одинаковую оптимальную политику. Этот подход использует предположение о том, что все агенты имеют одно и то же пространство действий и пространство наблюдений, и они также имеют общую глобальную награду. Этот подход особенно эффективен для коммуникационных действий, поскольку композиционный язык постоянно появляется среди всех участников игрового сценария. В варианте с общим набором сетей все агенты говорят на одном языке, в отличие от стандартного MADDPG, где каждый агент может интерпретировать один и тот же ориентир по-разному. Это особенно важно, когда сотрудничают более двух агентов, поскольку им нужно общаться на одном языке.
