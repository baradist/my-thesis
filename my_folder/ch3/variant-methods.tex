\section{Варианты алгоритмов}

\subsection{MADDPG с декомпозированным вознаграждением (Decomposed Reward)}

Предполагается сценарий игры, в котором агенты выполняют относительно независимые под-действия, каждое под-действие может воздействовать на среду и приводить к вознаграждению, которое отделено от глобального вознаграждения. Идея декомпозирования награды MADDPG в том, что для каждого под-действия может быть сформулирована независимая политика. Каждый агент, имеет \textit{n} независимых наборов сетей \textit{актор-критиков}, каждый из которых соответствует одному виду действия. Ожидается, что политики для отдельных видов действий могут быть оптимизированы путём обучения соответствующих групп критиков.

В этой работе планируется использовать этот вариант в сценарии \textit{Simple Reference}.

\subsection{MADDPG с общим мозгом}

Другой вариант MADDPG вдохновлён \cite{mordatch2017emergence}. В этом варианте есть только один набор сетей \textit{акторов-критиков}, который используется всеми агентами. Это подразумевает, что все агенты имеют одинаковую оптимальную политику. Этот подход использует предположение о том, что все агенты имеют одно и то же пространство действий и пространство наблюдений, и они также имеют общую глобальную награду. Этот подход особенно эффективен для коммуникационных действий, поскольку композиционный язык постоянно появляется среди всех участников игрового сценария. В варианте с общим набором сетей все агенты говорят на одном языке, в отличие от стандартного MADDPG, где каждый агент может интерпретировать один и тот же ориентир по-разному. Это особенно важно, когда сотрудничают более двух агентов, поскольку им нужно общаться на одном языке.
