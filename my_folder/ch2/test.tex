извлекается из $D$, а критик каждого агента обновляется путём минимизации потерь:
И актор и критик обновляется семплированым градиентом политики:
ванного Simple Speaker Listener для приближения к $Q$-функции и отдельных децентрализованных акторов для оптимизации политики каждого агента $\pi (h)$. \cite{foerster2017counterfactual}

Задачи отражают \textit{поэтапное достижение цели, при этом уточняют границы проводимого исследования}.
 (должно быть \textit{не менее четырех задач, но не более шести задач}):
использовать только в \textbf{\textit{названиях подпараграфов (пунктов)}} ииспользовать только в \textbf{\textit{названиях подпараграфов (пунктов)}} и
Введение \textit{не должно превышать 4 страницы}.
Введение \textit{не-должно превышать страницы}.
