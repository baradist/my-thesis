\chapter{Применяемые методы} \label{ch3}

Основным алгоритмом, применяемым в этой работе, является \hyperref[acr:ddpg]{градиент глубокой детерминированной политики} (Deep Deterministic Policy Gradient, DDPG), который использует архитектуру актор-критик и может работать в пространстве непрерывных действий. Поскольку сценарий игры предполагает совместную работу нескольких агентов, используется мультиагентная модификация алгоритма DDPG~---~\hyperref[acr:maddpg]{мультиагентный градиент глубокой детерминированной политики} (MADDPG). Основное его отличие, делающее его более подходящим для работы с несколькими агентами, состоит в том, что MADDPG имеет N наборов ИНС критиков-акторов, где N соответствует количеству агентов в сценарии, благодаря чему каждый агент имеет свой собственный механизм оптимизации политики. В оригинальном дизайне \cite{lowe2017multiagent} это сделано для адаптации как к совместной работе, так и к конкуренции между агентами.

Поскольку эта работа фокусируется на совместной работе нескольких агентов, тестируются несколько вариантов MADDPG, подходящих для совместной работы.

\input{my_folder/ch3/principal-method}
\input{my_folder/ch3/applied-techniques}
\input{my_folder/ch3/variant-methods}

\section{Выводы}

В этой главе были рассмотрены методы, которые были выбраны для исследования. Именно эти методы, подходы и алгоритмы были задействованы в экспериментах, речь о которых пойдёт в следующей главе. 

Сценарии, в которых производились эксперименты, уже упоминались в \hyperref[intro:sec2]{1.2 Сценарии}. Более подробно они так же будут рассмотрены в следующей главе.

\newpage
