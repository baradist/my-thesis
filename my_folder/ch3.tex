\chapter{Применяемые методы} \label{ch3}

Основным алгоритмом, применяемым в этой работе, является \hyperref[acr:ddpg]{\textit{градиент глубокой детерминированной политики}} (Deep Deterministic Policy Gradient, DDPG), который использует архитектуру актор-критик и может работать в пространстве непрерывных действий. Поскольку сценарий игры предполагает совместную работу нескольких агентов, используется мультиагентная модификация алгоритма DDPG --- \hyperref[acr:maddpg]{\textit{мультиагентный градиент глубокой детерминированной политики}} (MADDPG). Основное его отличие, делающее его более подходящим для работы с несколькими агентами, состоит в том, что MADDPG имеет N наборов ИНС критиков-акторов, где N соответствует количеству агентов в сценарии, благодаря чему, каждый агент имеет свой собственный механизм оптимизации политики. В оригинальном дизайне \cite{lowe2017multiagent} это сделано для адаптации как к совместной работе, так и к конкуренции между агентами.

Поскольку эта работа фокусируется на совместной работе нескольких агентов, тестируются несколько вариантов MADDPG, подходящих для совместной работы.

\input{my_folder/ch3/principal-method}
\input{my_folder/ch3/applied-techniques}
\input{my_folder/ch3/variant-methods}

\newpage
